{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235581f",
   "metadata": {},
   "source": [
    "The text of [*Alice in Wonderland*](https://www.gutenberg.org/files/11/11-0.txt) is taken from Project Gutenberg.\n",
    "\n",
    "The list of [English stop words](https://www.kaggle.com/datasets/heeraldedhia/stop-words-in-28-languages) is taken from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the text as a string in the 'corpus' variable, and the list of stop words also as string in the 'stopstring' variable.\n",
    "fhandle = open('text_files/english_stopwords.txt', 'r', encoding = 'UTF-8')\n",
    "stopstring = fhandle.read()\n",
    "\n",
    "thandle = open('text_files/Alice_in_Wonderland.txt', 'r', encoding = 'UTF-8')\n",
    "corpus = thandle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing both source files\n",
    "fhandle.close()\n",
    "thandle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the text and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising the text and the stop words\n",
    "stoplist = word_tokenize(stopstring)\n",
    "text = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(str_list, word_set):\n",
    "    '''\n",
    "    Removes stop words from the list of words.\n",
    "    '''\n",
    "    new_list = list(filter(lambda x: x not in word_set, str_list))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punc(str_list):\n",
    "    '''\n",
    "    Removes punctuation from the the list of words.\n",
    "    '''\n",
    "    punc = string.punctuation + \",.;’_—“”‘\"\n",
    "    new_list = []\n",
    "    for elem in str_list:\n",
    "        new_list.append(elem.strip(punc).strip())\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(str_list, stopwords_set):\n",
    "    '''\n",
    "    Removes stop words, punctuation and lemmatizes words.\n",
    "    Returns a list of words.\n",
    "    '''\n",
    "    final = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_list = list(map(str.lower, strip_punc(str_list)))\n",
    "    new_list = remove_stopwords(new_list, stopwords_set)\n",
    "    for elem in new_list:\n",
    "        final.append(lemmatizer.lemmatize(elem))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating frecuency & probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(str_list):\n",
    "    '''\n",
    "    Returns a dictionary with words as keys and their frecuency as values.\n",
    "    '''\n",
    "    words_dict = {}\n",
    "    for elem in str_list:\n",
    "        if elem != '':\n",
    "            if elem not in words_dict:\n",
    "                words_dict[elem] = 1\n",
    "            else:\n",
    "                words_dict[elem] += 1\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probability(dictionary, length):\n",
    "    '''\n",
    "    Returns a dictionary with words as keys and their probabilityies as values.\n",
    "    '''\n",
    "    return dict(map(lambda x: (x[0], x[1]/length), dictionary.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the dictionary in order to get an ordered histogram later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(dictionary, reversed_order):\n",
    "    '''\n",
    "    Returns a list of tuples with key-value pairs sorted by frecuency.\n",
    "    '''\n",
    "    list_d = list(dictionary.items())                                          # converting dictionary into a list\n",
    "    list_swop = []                                       \n",
    "    for key, value in list_d:                           \n",
    "        list_swop.append((value, key))                                         # adding key and value tuples in swopped order\n",
    "    list_sorted = sorted(list_swop, reverse=reversed_order)\n",
    "    return list_sorted            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(sorted_list,top_n):\n",
    "    '''\n",
    "    Prints the first top_n pairs of word-probability.\n",
    "    '''\n",
    "    short = sorted_list[:top_n]\n",
    "    print('{:<20} {:>22}\\n'.format('Word', 'Percent'))\n",
    "    for k, v in short:\n",
    "        print('{:<20} {:>20}%'.format(v,round(k*100,2)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = normalize(text, stopwords_set)                                 #getting a clean list of semantically important words\n",
    "words_frecuency = count_words(clean_text)                                   #getting a dictionary with words' frequencies\n",
    "length = len(clean_text)\n",
    "prob = word_probability(words_frecuency, length)                            #getting a dictionary with words' probabilities\n",
    "histogram = sort_dict_by_value(prob, True)                                  #sorting by words' probabilities in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d04c3",
   "metadata": {},
   "source": [
    "Let's display top 10 most frequent words with their corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                                Percent\n",
      "\n",
      "alice                                2.72%\n",
      "queen                                0.53%\n",
      "time                                  0.5%\n",
      "king                                 0.44%\n",
      "turtle                               0.42%\n",
      "head                                  0.4%\n",
      "mock                                 0.39%\n",
      "hatter                               0.39%\n",
      "gryphon                              0.38%\n",
      "voice                                0.34%\n"
     ]
    }
   ],
   "source": [
    "display(histogram,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "70e391745944b3d272f4a421ff0759b1be2ea133586440d844e4b505c00a2e61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
